{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Matthew Woo\n",
        "20236203**\n"
      ],
      "metadata": {
        "id": "HwkeghNE_-Ur"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-SOCfgJtHJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82994a2-a8a6-4fb4-950a-b228f7612203"
      },
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import matplotlib.colors as mcolors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b53f5efa330b>:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-notebook')\n",
            "<ipython-input-2-b53f5efa330b>:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-whitegrid')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0cEpP6BtPSO"
      },
      "source": [
        "class Action(IntEnum):\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "action_to_str = {\n",
        "    Action.up : \"up\",\n",
        "    Action.right : \"right\",\n",
        "    Action.down : \"down\",\n",
        "    Action.left : \"left\",\n",
        "}\n",
        "\n",
        "action_to_offset = {\n",
        "    Action.up : (-1, 0),\n",
        "    Action.right : (0, 1),\n",
        "    Action.down : (1, 0),\n",
        "    Action.left : (0, -1),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvTUR66UtPUk"
      },
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=1.0, danger=[], danger_value=-1.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [[0,0,0,0] for _ in range(height*width)] # Initialize state values.\n",
        "        self._grid_policies = [np.random.randint(0,4) for _ in range(height * width)]\n",
        "        self._returns = [[[],[],[],[]] for _ in range(height*width)]\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1 # Ensure valid noise value.\n",
        "        self.create_next_values() # Initialize the next state values.\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [[0,0,0,0] for _ in range(self.height*self.width)]\n",
        "        self.create_next_values()\n",
        "\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state >= 0 and state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state_r >= 0 and state_r < self._height and state_c >= 0 and state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #[1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "        a = self.get_actions(state)\n",
        "\n",
        "        if action in a:\n",
        "            if action==0:\n",
        "                return state-self._width\n",
        "            elif action==1:\n",
        "                return state+1\n",
        "            elif action==2:\n",
        "                return state+self._width\n",
        "            else:\n",
        "                return state-1\n",
        "        return state\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        #To Do:\n",
        "        return (state == self._goal_cell) or (state in self._danger_cells)\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        non_term = []\n",
        "        for x in range(self._width*self._height):\n",
        "            if not self.is_terminal(x) and x not in self._blocked_cells:\n",
        "                non_term.append(x)\n",
        "        return non_term\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state\n",
        "        \"\"\"\n",
        "        #TO DO:\n",
        "        #Returns only actions that go to non block states\n",
        "\n",
        "        #Obtaining row and col of all block states\n",
        "        block = []\n",
        "        for i in self._blocked_cells:\n",
        "            r,c = self._state_to_rc(i)\n",
        "            block.append([r,c])\n",
        "\n",
        "        #Loop through all actions, check if the taking action is within bounds\n",
        "        valid_act = []\n",
        "        r,c = self._state_to_rc(state)\n",
        "        for x in range(0,4):\n",
        "            a = action_to_offset[x]\n",
        "            if self._inbounds_rc(r+a[0],c+a[1]) and [r+a[0],c+a[1]] not in block:\n",
        "                valid_act.append(x)\n",
        "        return valid_act\n",
        "\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        # Reward is non-zero for danger or goal\n",
        "        #TO DO:\n",
        "        if state == self._goal_cell:\n",
        "            return 1\n",
        "        elif state in self._danger_cells:\n",
        "            return -1\n",
        "        return -0.1\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2}\".format(action_to_str[self._grid_policies[cell]])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_on_policy_explore(gw, discount):\n",
        "    #Initialize action state estimate and policy for all states\n",
        "    #Already done with initialization of gridworld\n",
        "\n",
        "    #Randomly choosing an action different from the initial policy is an exploring start\n",
        "    episode_count = 0\n",
        "    while episode_count<4000:\n",
        "        #Can choose the starting state or make it random if you uncomment line below\n",
        "        s = np.random.choice(gw.get_states(), 1)[0]\n",
        "        a = np.random.randint(0,4)\n",
        "        iter = 0\n",
        "        episode = []\n",
        "        #Episode generation\n",
        "        while iter < 30 and not gw.is_terminal(s):\n",
        "            nextState = gw._state_from_action(s,a)\n",
        "            rew = gw.get_reward(nextState)\n",
        "            #Tuple storing the current state, action and reward for taking action\n",
        "            episode.append((s,a,rew))\n",
        "            s = nextState\n",
        "            a = gw._grid_policies[s]\n",
        "            iter += 1\n",
        "        #Episode traversal\n",
        "        g = 0\n",
        "        for t in range(len(episode)-1,-1,-1):\n",
        "            g = discount*g + episode[t][2]\n",
        "            ts = episode[t][0]\n",
        "            ta = episode[t][1]\n",
        "            prev = False\n",
        "            #First Visit\n",
        "            #If the state and action at the current t iteration show up in an earlier t then skip the current iteration\n",
        "            for check in range(0,t):\n",
        "                if ts == episode[check][0] and ta == episode[check][1]:\n",
        "                    prev = True\n",
        "                    break\n",
        "            if not prev:\n",
        "                gw._returns[ts][ta].append(g)\n",
        "                gw._grid_values[ts][ta] = np.mean(gw._returns[ts][ta])\n",
        "                gw._grid_policies[ts] = np.random.choice(np.where(gw._grid_values[ts] == np.array(gw._grid_values[ts]).max())[0])\n",
        "        episode_count += 1"
      ],
      "metadata": {
        "id": "6XvSXT8FVGp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gw = GridWorld(height=3,width=4,goal=3,danger=[7],blocked=[5],noise=0.0)\n",
        "mc_on_policy_explore(test_gw,0.95)\n",
        "print(test_gw)\n",
        "for x in test_gw._grid_values:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsThcDBqpeL8",
        "outputId": "aa57f5d1-b18b-44ce-93e0-cba727f50fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ri     ri     ri   GOAL \n",
            "    up   ----     up  -1.00 \n",
            "    ri     ri     up     le \n",
            "\n",
            "[0.5356882359818034, 0.705138084902602, 0.426349373551596, 0.5507082937574255]\n",
            "[0.6879224014957074, 0.8499999999999996, 0.7074999999999998, 0.5515206973835179]\n",
            "[0.8499999999999995, 1.0, 0.7074999999999997, 0.7074999999999997]\n",
            "[0, 0, 0, 0]\n",
            "[0.5639927515289785, 0.4206262480687948, 0.28954339595186346, 0.364328497049916]\n",
            "[0, 0, 0, 0]\n",
            "[0.8499999999999999, -1.0, 0.5721249999999999, 0.686788886617144]\n",
            "[0, 0, 0, 0]\n",
            "[0.37178329165752455, 0.4361998726478665, 0.30349314000364036, 0.290255729730339]\n",
            "[0.4266403799320775, 0.5700533881131529, 0.40806840583956644, 0.30336712278351097]\n",
            "[0.7074999999999998, 0.4435187500000001, 0.5721249999999999, 0.39961203665629896]\n",
            "[-1.0, 0.4435187500000001, 0.4435187500000001, 0.5721249999999999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_on_policy_no_exp(gw, discount, epsilon):\n",
        "    episode_count = 0\n",
        "    gw._grid_policies = [[(1-epsilon)+(epsilon/4),epsilon/4,epsilon/4,epsilon/4] for _ in range(gw._height*gw._width)]\n",
        "    while episode_count < 4000:\n",
        "        #Can choose the starting state or make it random if you uncomment line below\n",
        "        #s = np.random.choice(gw.get_states(), 1)[0]\n",
        "        s = 8\n",
        "        a = np.random.choice([0,1,2,3],1,[gw._grid_policies[s]])[0]\n",
        "        iter=0\n",
        "        episode=[]\n",
        "        #Episode Generation\n",
        "        while iter < 30 and not gw.is_terminal(s):\n",
        "            nextState = gw._state_from_action(s,a)\n",
        "            rew = gw.get_reward(nextState)\n",
        "            #Tuple storing the current state, action and reward for taking action\n",
        "            episode.append((s,a,rew))\n",
        "            s = nextState\n",
        "            a = np.random.choice([0,1,2,3],1,[gw._grid_policies[s]])[0]\n",
        "            iter += 1\n",
        "        g = 0\n",
        "        for t in range(len(episode)-1,-1,-1):\n",
        "            g = (g*discount) + episode[t][2]\n",
        "            ts = episode[t][0]\n",
        "            ta = episode[t][1]\n",
        "            prev = False\n",
        "            #First Visit\n",
        "            #If the state and action at the current t iteration show up in an earlier t then skip the current iteration\n",
        "            for check in range(0,t):\n",
        "                if ts == episode[check][0] and ta == episode[check][1]:\n",
        "                    prev = True\n",
        "                    break\n",
        "            if not prev:\n",
        "                gw._returns[ts][ta].append(g)\n",
        "                gw._grid_values[ts][ta] = sum(gw._returns[ts][ta])/len(gw._returns[ts][ta])\n",
        "                #Choosing the action/index with the greatest state action value\n",
        "                A = np.random.choice(np.where(gw._grid_values[ts] == np.array(gw._grid_values[ts]).max())[0])\n",
        "                #action equal to A will be the greedy action while the rest are non greedy\n",
        "                for x in range(4):\n",
        "                    if x == A:\n",
        "                        gw._grid_policies[ts][x] = 1 - epsilon + epsilon/4\n",
        "                    else:\n",
        "                        gw._grid_policies[ts][x] = epsilon/4\n",
        "\n",
        "        episode_count += 1\n",
        "    #Finalizing policy\n",
        "    for x in range(gw._height*gw._width):\n",
        "          gw._grid_policies[x] = np.random.choice(np.where(gw._grid_policies[x] == np.array(gw._grid_policies[x]).max())[0])"
      ],
      "metadata": {
        "id": "PK3oBuzGnnyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_gw2 = GridWorld(height=3,width=4,goal=3,danger=[7],blocked=[5],noise=0.0)\n",
        "mc_on_policy_no_exp(tiny_gw2,0.95,0.25)\n",
        "print(tiny_gw2)\n",
        "for x in tiny_gw2._grid_values:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGpLFPknAZa7",
        "outputId": "d51079c2-56ae-4926-dfb8-bb152c50970e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ri     ri     ri   GOAL \n",
            "    up   ----     up  -1.00 \n",
            "    ri     ri     up     le \n",
            "\n",
            "[-0.9484234635345208, -0.6893232965213318, -1.0773279005849377, -0.9346779375753211]\n",
            "[-0.6123315052003212, -0.23904415044948063, -0.600420441262076, -0.8604526945993399]\n",
            "[-0.17429856773458044, 1.0, -0.7488913228116552, -0.6358650397996026]\n",
            "[0, 0, 0, 0]\n",
            "[-1.0296879753744936, -1.1365010759443033, -1.2066473344775723, -1.1501596668587186]\n",
            "[0, 0, 0, 0]\n",
            "[-0.2177473525778785, -1.0, -0.9552616652760914, -0.7726254593579693]\n",
            "[0, 0, 0, 0]\n",
            "[-1.2642113943142512, -1.2639810225401198, -1.2904598535678977, -1.292976241390617]\n",
            "[-1.1829496151576884, -1.0793197688634775, -1.1783284120501356, -1.2206759232276572]\n",
            "[-0.8094941656520858, -1.048403075329322, -1.0183028110227192, -1.09714720558039]\n",
            "[-1.0, -1.0342010347036497, -1.0328177301398918, -0.9832135880787689]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def off_policy_mc_prediction(gw, discount, epsilon, policy=None):\n",
        "    C = [[0,0,0,0] for _ in range(gw._height*gw._width)]\n",
        "    gw._grid_policies = [[1,0,0,0] for _ in range(gw._height*gw._width)]\n",
        "    if policy != None:\n",
        "        gw._grid_policies = policy.copy()\n",
        "    episode_count = 0\n",
        "\n",
        "    #Episode generation\n",
        "    while episode_count < 4000:\n",
        "        b = [[0.25,0.25,0.25,0.25] for _ in range(gw._height*gw._width)]\n",
        "        episode = []\n",
        "        iter = 0\n",
        "        #Can choose the starting state or make it random if you uncomment line below\n",
        "        #s = np.random.choice(gw.get_states(), 1)[0]\n",
        "        s=8\n",
        "        a = np.random.choice([0,1,2,3],1,b[s])[0]\n",
        "        while iter<30 or not gw.is_terminal(s):\n",
        "            newstate = gw._state_from_action(s,a)\n",
        "            rew = gw.get_reward(newstate)\n",
        "            #Tuple storing the current state, action and reward for taking action\n",
        "            episode.append((s,a,rew))\n",
        "            s=newstate\n",
        "            a=np.random.choice([0,1,2,3],1,b[s])[0]\n",
        "            iter+=1\n",
        "        #Episode traversal\n",
        "        g=0\n",
        "        w=1\n",
        "        for t in range(len(episode)-1,-1,-1):\n",
        "            if w == 0:\n",
        "                break\n",
        "            ts = episode[t][0]\n",
        "            ta = episode[t][1]\n",
        "            g = (discount*g) + episode[t][2]\n",
        "            C[ts][ta] = C[ts][ta] + w\n",
        "            gw._grid_values[ts][ta] = gw._grid_values[ts][ta] + (w/C[ts][ta])*(g - gw._grid_values[ts][ta])\n",
        "            w = w*(gw._grid_policies[ts][ta]/b[ts][ta])\n",
        "        episode_count+=1\n",
        "\n",
        "    #Finalizing policy\n",
        "    for x in range(len(gw._grid_policies)):\n",
        "        gw._grid_policies[x] = np.random.choice(np.where(gw._grid_values[x] == np.array(gw._grid_values[x]).max())[0])"
      ],
      "metadata": {
        "id": "TCh_pemM4jXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_gw3 = GridWorld(height=3,width=4,goal=3,danger=[7],blocked=[5],noise=0.0)\n",
        "policy = [[0,1,0,0],[0,1,0,0],[0,1,0,0],[0,0,0,0],[1,0,0,0],[0,0,0,0],[1,0,0,0],[0,0,0,0],[1,0,0,0],[0,1,0,0],[1,0,0,0],[0,0,0,1]]\n",
        "off_policy_mc_prediction(tiny_gw3,0.95,0.25,policy)\n",
        "print(tiny_gw3)\n",
        "for x in tiny_gw3._grid_values:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIZj6Q6DLx0h",
        "outputId": "e46053bb-cb89-4eda-9f75-35a713ed0925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ri     ri     ri   GOAL \n",
            "    up   ----     up  -1.00 \n",
            "    up     ri     up     le \n",
            "\n",
            "[0.572125, 0.7075, 0.44351874999999996, 0.572125]\n",
            "[0.7075, 0.85, 0.7075, 0.572125]\n",
            "[0.85, 1.0, 0.7075, 0.7075]\n",
            "[1.0, 1.0, -1.0, 0.85]\n",
            "[0.572125, 0.44351874999999996, 0.32134281249999996, 0.44351874999999996]\n",
            "[0, 0, 0, 0]\n",
            "[0.85, -1.0, 0.572125, 0.7075]\n",
            "[1.0, -1.0, 0.44351874999999996, 0.7075]\n",
            "[0.44351874999999996, 0.44351874999999996, 0, 0.32134281249999996]\n",
            "[0.44351874999999996, 0.572125, 0.44351874999999996, 0.32134281249999996]\n",
            "[0.7075, 0.44351874999999996, 0.572125, 0.44351874999999996]\n",
            "[-1.0, 0.44351874999999996, 0.44351874999999996, 0.572125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def off_policy_mc_estimate(gw, discount, epsilon):\n",
        "    C = [[0,0,0,0] for _ in range(gw._height*gw._width)]\n",
        "\n",
        "    episode_count = 0\n",
        "    while episode_count < 4000:\n",
        "        #Creating random epsilon soft policy b\n",
        "        b = []\n",
        "        for state in range(gw._height*gw._width):\n",
        "            temp = [0,0,0,0]\n",
        "            #Just appending an all 0 list to keep state index/order correct\n",
        "            if gw.is_terminal(state):\n",
        "                b.append(temp)\n",
        "                continue\n",
        "            #maxact = np.random.choice(np.where(gw._grid_values[x] == np.array(gw._grid_values[x]).max())[0])\n",
        "            maxact = np.random.randint(0,4)\n",
        "            for act in range(4):\n",
        "                if maxact == act:\n",
        "                    temp[act] = (1-epsilon) + epsilon/4\n",
        "                else:\n",
        "                    temp[act] = epsilon/4\n",
        "            b.append(temp)\n",
        "        episode = []\n",
        "        iter = 0\n",
        "        #Can choose the starting state or make it random if you uncomment line below\n",
        "        #s = np.random.choice(gw.get_states(), 1)[0]\n",
        "        s=8\n",
        "        a = np.random.choice(np.where(b[s] == np.array(b[s]).max())[0])\n",
        "\n",
        "        while iter<30 and not gw.is_terminal(s):\n",
        "            newstate = gw._state_from_action(s,a)\n",
        "            rew = gw.get_reward(newstate)\n",
        "            #Tuple storing the current state, action and reward for taking action\n",
        "            episode.append((s,a,rew))\n",
        "            s = newstate\n",
        "            #Selecting action based on b policy\n",
        "            a = np.random.choice([0,1,2,3],1,b[s])[0]\n",
        "            iter+=1\n",
        "\n",
        "        g = 0\n",
        "        w = 1\n",
        "        for t in range(len(episode)-1,-1,-1):\n",
        "            ts = episode[t][0]\n",
        "            ta = episode[t][1]\n",
        "            g = (discount*g) + episode[t][2]\n",
        "            C[ts][ta] = C[ts][ta] + w\n",
        "            gw._grid_values[ts][ta] = gw._grid_values[ts][ta] + (w/C[ts][ta])*(g-gw._grid_values[ts][ta])\n",
        "            gw._grid_policies[ts] = np.random.choice(np.where(gw._grid_values[ts]==np.array(gw._grid_values[ts]).max())[0])\n",
        "            if ta != gw._grid_policies[ts]:\n",
        "                break\n",
        "            w = w*(1/b[ts][ta])\n",
        "        episode_count+=1\n"
      ],
      "metadata": {
        "id": "TN3i8rIddezH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_gw4 = GridWorld(height=3,width=4,goal=3,danger=[7],blocked=[5],noise=0.0)\n",
        "off_policy_mc_estimate(tiny_gw4,0.95,0.25)\n",
        "print(tiny_gw4)\n",
        "for x in tiny_gw4._grid_values:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u5sv3kVd4Rt",
        "outputId": "af8fdeba-45b9-4b00-fa36-ded0f0069966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ri     ri     ri   GOAL \n",
            "    up   ----     up  -1.00 \n",
            "    ri     ri     up     le \n",
            "\n",
            "[0.566966771178729, 0.6977268276372671, 0.4398867384420996, 0.5391836473725828]\n",
            "[0.6971176657770882, 0.8405606077149852, 0.6857862269460351, 0.5693336176944059]\n",
            "[0.8410572049239677, 1.0, 0.6935869812225934, 0.6985047198080485]\n",
            "[0, 0, 0, 0]\n",
            "[0.5662339947000565, 0.44151419768367606, 0.3203595816891152, 0.4420906531754437]\n",
            "[0, 0, 0, 0]\n",
            "[0.8397374106697696, -1.0, 0.5367816522145943, 0.6931820680238764]\n",
            "[0, 0, 0, 0]\n",
            "[0.42440771148415046, 0.4323545621949361, 0.32059525309300374, 0.2751741859878291]\n",
            "[0.4338761788058121, 0.5575706765689029, 0.4091198859548373, 0.314322534605197]\n",
            "[0.693102711563939, 0.4430415489908973, 0.5623045361127288, 0.439869909431457]\n",
            "[-1.0, 0.40553908195395316, 0.3989086348369919, 0.5607596072860089]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(tiny_gw3._grid_values)\n",
        "#print(tiny_gw4._grid_values)\n",
        "\n",
        "#Calculating the difference between different algorithms final _grid_values\n",
        "\n",
        "for x in tiny_gw3.get_states():\n",
        "    print(np.subtract(tiny_gw3._grid_values[x],tiny_gw4._grid_values[x]))\n",
        "\n",
        "for x in tiny_gw3.get_states():\n",
        "    print(np.subtract(test_gw._grid_values[x],tiny_gw3._grid_values[x]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_xKuCS5ti8t",
        "outputId": "aea78334-3872-4d8b-e389-ae89b9186976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00475566 0.00764255 0.07973654 0.00680764]\n",
            "[0.00902498 0.00997288 0.01419473 0.01040637]\n",
            "[0.00878527 0.         0.00780841 0.01679128]\n",
            "[0.00913365 0.00211929 0.31245853 0.01399286]\n",
            "[0.01012888 0.         0.02091808 0.02574852]\n",
            "[0.00746041 0.0015551  0.00018705 0.15685407]\n",
            "[0.0194314  0.00740088 0.00559358 0.00024146]\n",
            "[0.00880727 0.00216908 0.00603008 0.01258298]\n",
            "[0.         0.00440608 0.01634396 0.0156649 ]\n",
            "[-0.09777597 -0.01067187 -0.16013694 -0.08349624]\n",
            "[-0.10247476 -0.00206557 -0.06533799 -0.20799677]\n",
            "[-0.02122238  0.         -0.03594006 -0.11148799]\n",
            "[-0.02172791 -0.11605391 -0.03044121 -0.11757464]\n",
            "[-0.00204799  0.         -0.0338382  -0.08269097]\n",
            "[-0.12934825 -0.01011963 -0.04835183 -0.04364201]\n",
            "[-0.03194457 -0.01237244 -0.02032294 -0.02655392]\n",
            "[-0.01124385 -0.05280323 -0.05594761 -0.03653308]\n",
            "[ 0.         -0.02928711 -0.0676976  -0.00845955]\n"
          ]
        }
      ]
    }
  ]
}